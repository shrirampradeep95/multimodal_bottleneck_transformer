# Multimodal Bottleneck Transformer (MBT)

A PyTorch-based implementation of the **Multimodal Bottleneck Transformer** for audio-visual classification. This project supports training and evaluating models on datasets like **AudioSet** and **VGGSound**, using audio, video, or fused features with cross-modal bottleneck fusion.

## ðŸ§  Key Features

- Multimodal fusion with bottleneck tokens (MBT)
- Supports AudioSet (multi-label) and VGGSound (single-label)
- Training options for audio-only, video-only, or both
- Plug-and-play architecture using PyTorch modules
- Visualization of training loss and evaluation metrics
---

## ðŸ“‚ Project Structure

multimodal_bottleneck_transformer/ â”‚ â”œâ”€â”€ utils/ â”‚ â”œâ”€â”€ preprocessing/ â”‚ â”‚ â”œâ”€â”€ audioset_data_preprocessing.py â”‚ â”‚ â”œâ”€â”€ vgg_sound_data_preprocessing.py â”‚ â”‚ â””â”€â”€ video_data_preprocessing.py â”‚ â””â”€â”€ train_eval/ â”‚ â”œâ”€â”€ trainer_evaluator.py â”‚ â””â”€â”€ generate_plots.py â”‚ â”œâ”€â”€ utils/model/ â”‚ â”œâ”€â”€ audio_model.py â”‚ â”œâ”€â”€ video_model.py â”‚ â”œâ”€â”€ bottleneck_fusion.py â”‚ â””â”€â”€ multimodal_transformer.py â”‚ â”œâ”€â”€ runner.py (or mbt_runner.py) # Entry point â””â”€â”€ README.md
